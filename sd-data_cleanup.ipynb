{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the features: ['sex' 'age' 'height' 'weight' 'waistline' 'sight_left' 'sight_right'\n",
      " 'hear_left' 'hear_right' 'SBP' 'DBP' 'BLDS' 'tot_chole' 'HDL_chole'\n",
      " 'LDL_chole' 'triglyceride' 'hemoglobin' 'urine_protein'\n",
      " 'serum_creatinine' 'SGOT_AST' 'SGOT_ALT' 'gamma_GTP' 'SMK_stat_type_cd'\n",
      " 'DRK_YN']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "filename = 'sd-data_raw.csv'\n",
    "data = pd.read_csv(filename)\n",
    "\n",
    "raw_data = data.values # converting from pandas dataframe to numpy array\n",
    "attribute_names = np.asarray(data.columns) # extracting the attribute names\n",
    "print(\"These are the features:\",attribute_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(raw_data) # turning data into numpy array\n",
    "\n",
    "# For the gender attribute we replace 'male' with the number 0 and 'female' with 1:\n",
    "data[data == 'Male'] = 0 \n",
    "data[data == 'Female'] = 1\n",
    "# Same goes for drink yes/no. yes with 1 and no with 0\n",
    "data[data == 'Y'] = 1 \n",
    "data[data == 'N'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 'sight_left' and 'sight_right' the scale goes from 0.1 - 2.5 depending on how good your sight is\n",
    "# However 9.9 means blind. To handle this, we just change every 9.9 to a 0, given that 0 would be the worst possible sight \n",
    "data[:, 5][data[:, 5] == 9.9] = 0\n",
    "data[:, 6][data[:, 6] == 9.9] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying 1-out-of-K on some of the discrete variables. \n",
    "# hearing_left and hearing_right, K=2\n",
    "# urine_protien, K=6 and smoking state, K=3\n",
    "one_out_of_K_cols_dict = {'encoding1':'hear_left', 'encoding2':'hear_right', 'encoding3':'urine_protein', 'encoding4':'SMK_stat_type_cd'}\n",
    "\n",
    "# getting indices of collums that needs encoding\n",
    "indices_list = [index for index, element in enumerate(attribute_names) if element in one_out_of_K_cols_dict.values()]\n",
    "\n",
    "key_iterator = iter(one_out_of_K_cols_dict)\n",
    "\n",
    "# For loop for creating encoding matrix for all keys in one_out_of_K_cols_dict\n",
    "for i in range(len(indices_list)):\n",
    "    K = int(data[:,indices_list[i]].max())+1\n",
    "    encoding = np.zeros((data[:,indices_list[i]].size, K))\n",
    "    encoding[np.arange(data[:,indices_list[i]].size), data[:,indices_list[i]].astype(int)] = 1\n",
    "    # deleting first column, because values incoded goes from 1:n and not 0:n, so K=K+1, for this to work.\n",
    "    encoding = encoding[:,1:]\n",
    "    # Storing encoding as value in dict\n",
    "    one_out_of_K_cols_dict[str(next(key_iterator))] = encoding\n",
    "\n",
    "# Deleting old data columns\n",
    "for j in range(len(indices_list)):\n",
    "    data = np.delete(data, indices_list[j]-j, axis=1)\n",
    "\n",
    "# Remember that last key (encoding for 'SMK_stat_type_cd') will be the first colums in data matrix\n",
    "for key in one_out_of_K_cols_dict:\n",
    "    for k in range(one_out_of_K_cols_dict[str(key)].shape[1]):\n",
    "        data = np.insert(data, k, one_out_of_K_cols_dict[str(key)][:,k], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0. ,    0. ,    0. ,    0. ,    0. ,    0. ,    0. ,    0. ,\n",
       "           0. ,    0. ,    0. ,    0. ,    0. ,    0. ,   20. ,  130. ,\n",
       "          25. ,    8. ,    0. ,    0. ,   67. ,   32. ,   25. ,   30. ,\n",
       "           1. ,    1. ,    1. ,    1. ,    0.1,    1. ,    1. ,    1. ,\n",
       "           0. ],\n",
       "       [   1. ,    1. ,    1. ,    1. ,    1. ,    1. ,    1. ,    1. ,\n",
       "           1. ,    1. ,    1. ,    1. ,    1. ,    1. ,   85. ,  190. ,\n",
       "         140. ,  999. ,    2.5,    2.5,  273. ,  185. ,  852. , 2344. ,\n",
       "        8110. , 5119. , 9490. ,   25. ,   98. , 9999. , 7210. ,  999. ,\n",
       "           1. ]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "min_max_matrix = np.zeros((2, data.shape[1]), dtype=float)\n",
    "\n",
    "# to get an overview of where min-max scaling methods should be applied, \n",
    "# we create a matrix containing the minimum and maximum value for each attribute\n",
    "\n",
    "for i in range(data.shape[1]):\n",
    "    min_max_matrix[0,i] = min(data[:,i])\n",
    "    min_max_matrix[1,i] = max(data[:,i])\n",
    "\n",
    "min_max_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1.]])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# standardize data\n",
    "for i in range(data.shape[1]):\n",
    "    if max(data[:,i]) != 1.:\n",
    "        # Check den her formel\n",
    "        newdata = (data[:,i]-min(data[:,i]))/(max(data[:,i])-min(data[:,i]))\n",
    "        data[:,i] = np.round(newdata.astype(float), decimals=6)\n",
    "\n",
    "for i in range(data.shape[1]):\n",
    "    min_max_matrix[0,i] = min(data[:,i])\n",
    "    min_max_matrix[1,i] = max(data[:,i])\n",
    "\n",
    "min_max_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0001"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checks that 6 decimal when using np.round() is enough for even the smallets values in data\n",
    "non_zero_values = data[data != 0]\n",
    "np.min(non_zero_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exportdata = pd.DataFrame(data)\n",
    "#exportdata.to_csv('sd-data_cleanedv4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there issues with outliers in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the attributes seem to be normally distributed?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are the variables correlated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does the primary machine learning modeling aim appear to be feasible based on your visualizations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The amount of variation explained as a function of the number of PCA components included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the principal directions of the considered PCA components (either find a way to plot them or \n",
    "# interpret them in terms of the features),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data projected onto the considered principal components."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
